{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRAPH ATTENTION NETWORK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code is a simplified version of a Graph Attention Network. It starts by implementing a single Graph Attention Layer. This layer takes as input the features of nodes in a graph and its adjacency matrix, then applies a linear transformation to the node features. This is used to calculate attention coefficients between each pair of connected nodes using a learnable attention mechanism, involving a Leaky ReLU activation. These attention coefficients are normalized using a softmax function, and a droupout for regularization is applied. Finally, it aggregates the features of each node neighbors based on these attention weights to produce new, potentially more informative, feature representations for each node. The forward method starts these steps, and the example usage demonstrates how to initialize the layer with specified input and output features, dropout probability, and Leaky ReLU alpha, and then perform a forward pass using simulated node features and an adjacency matrix to obtain the transformed output node features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATlayer(nn.Module):\n",
    "    #Simple PyTorch Implementation of GAT\n",
    "    def __init__(self):\n",
    "        super(GATlayer, self).__init__()\n",
    "    def forward(self, input, adj):\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "in_features = 5\n",
    "out_features = 2\n",
    "nb_nodes = 4\n",
    "\n",
    "#Xavier Paramiter Initializator (Avoids vanishing or exploding gradients)\n",
    "W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "nn.init.xavier_uniform_(W.data, gain =1.414)\n",
    "\n",
    "#Linear Transformation\n",
    "input = torch.rand(nb_nodes, in_features)\n",
    "h = torch.mm(input, W)\n",
    "N = h.size()[0]\n",
    "\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "#Attention Implementation\n",
    "a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "#Xavier Paramiter Initializator\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
    "print(a.shape)\n",
    "#Activation Function\n",
    "leakyrelu = nn.LeakyReLU(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 4]) torch.Size([4, 1])\n",
      "torch.Size([4, 4, 1])\n",
      "torch.Size([4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.2363, 1.3500, 1.2268, 1.1665],\n",
       "        [0.5699, 0.6836, 0.5603, 0.5001],\n",
       "        [1.3419, 1.4556, 1.3323, 1.2721],\n",
       "        [1.8977, 2.0114, 1.8882, 1.8279]], grad_fn=<AsStridedBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Attention Coefficients\n",
    "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2*out_features)\n",
    "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))\n",
    "print(a_input.shape, a.shape)\n",
    "print(torch.matmul(a_input,a).shape)\n",
    "print(torch.matmul(a_input,a).squeeze(2).shape)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "#Adjacency Matrix\n",
    "adj = torch.randint(2, (nb_nodes, nb_nodes))\n",
    "zero_vec = -9e15*torch.ones_like(e)\n",
    "print(zero_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1, 0],\n",
      "        [0, 1, 1, 1],\n",
      "        [0, 1, 1, 1],\n",
      "        [1, 0, 1, 1]]) \n",
      " tensor([[1.2363, 1.3500, 1.2268, 1.1665],\n",
      "        [0.5699, 0.6836, 0.5603, 0.5001],\n",
      "        [1.3419, 1.4556, 1.3323, 1.2721],\n",
      "        [1.8977, 2.0114, 1.8882, 1.8279]], grad_fn=<AsStridedBackward0>) \n",
      " tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15, -9.0000e+15]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-9.0000e+15,  1.3500e+00,  1.2268e+00, -9.0000e+15],\n",
       "        [-9.0000e+15,  6.8359e-01,  5.6032e-01,  5.0005e-01],\n",
       "        [-9.0000e+15,  1.4556e+00,  1.3323e+00,  1.2721e+00],\n",
       "        [ 1.8977e+00, -9.0000e+15,  1.8882e+00,  1.8279e+00]],\n",
       "       grad_fn=<WhereBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply Mask\n",
    "attention = torch.where(adj>0, e, zero_vec)\n",
    "print(adj,\"\\n\",e,\"\\n\",zero_vec)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3083, -0.7552],\n",
      "        [-0.4786, -0.9165],\n",
      "        [-0.4786, -0.9165],\n",
      "        [-0.7139, -1.0121]], grad_fn=<MmBackward0>) \n",
      " tensor([[-0.6325, -0.8479],\n",
      "        [-0.0020, -0.6057],\n",
      "        [-0.6548, -0.9243],\n",
      "        [-0.8641, -1.2816]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Normalize Attention Score\n",
    "attention = f.softmax(attention, dim=1)\n",
    "#Weighted Node Features\n",
    "h_prime = torch.matmul(attention, h)\n",
    "print(h_prime,\"\\n\",h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha=0.2, concat=True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.attn_fc = nn.Linear(2 * out_features, 1, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        #Initializes weights (Xavier Uniform initialization)\n",
    "        init.xavier_uniform_(self.linear.weight)\n",
    "        init.xavier_uniform_(self.attn_fc.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = self.linear(input)\n",
    "        N = h.size(0)\n",
    "\n",
    "        #Attention inputs\n",
    "        attn_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leaky_relu(self.attn_fc(attn_input).squeeze(2))\n",
    "\n",
    "        #Masked attention\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "\n",
    "        #Normalize attention scores\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "\n",
    "        #Weighted sum of neighbor features\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features Shape: torch.Size([3, 5])\n",
      "Adjacency Matrix:\n",
      " tensor([[1., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 1., 0.]])\n",
      "Output Features Shape: torch.Size([3, 2])\n",
      "Output Features:\n",
      " tensor([[ 0.6170,  1.4479],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.1414,  0.1540]], grad_fn=<EluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "in_feat = 5\n",
    "out_feat = 2\n",
    "dropout_prob = 0.6\n",
    "alpha_val = 0.2\n",
    "num_nodes = 3\n",
    "\n",
    "#Create GAT layer\n",
    "gat_layer = GATLayer(in_feat, out_feat, dropout_prob, alpha_val)\n",
    "\n",
    "#Create input features and adjacency matrix\n",
    "input_features = torch.randn(num_nodes, in_feat)\n",
    "adjacency_matrix = torch.randint(0, 2, (num_nodes, num_nodes)).float()\n",
    "\n",
    "#Forward pass (Feed input to network)\n",
    "output_features = gat_layer(input_features, adjacency_matrix)\n",
    "\n",
    "print(\"Input Features Shape:\", input_features.shape)\n",
    "print(\"Adjacency Matrix:\\n\", adjacency_matrix)\n",
    "print(\"Output Features Shape:\", output_features.shape)\n",
    "print(\"Output Features:\\n\", output_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
