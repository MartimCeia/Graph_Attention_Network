# MNIST NEURAL NETWORK


This code is a simplified version of a Graph Attention Network. It starts by implementing a single Graph Attention Layer. This layer takes as input the features of nodes in a graph and its adjacency matrix, then applies a linear transformation to the node features. This is used to calculate attention coefficients between each pair of connected nodes using a learnable attention mechanism, involving a Leaky ReLU activation. These attention coefficients are normalized using a softmax function, and a droupout for regularization is applied. Finally, it aggregates the features of each node neighbors based on these attention weights to produce new, potentially more informative, feature representations for each node. The forward method starts these steps, and the example usage demonstrates how to initialize the layer with specified input and output features, dropout probability, and Leaky ReLU alpha, and then perform a forward pass using simulated node features and an adjacency matrix to obtain the transformed output node features.
